{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writeup Template\n",
    "\n",
    "---\n",
    "\n",
    "**Advanced Lane Finding Project**\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./Pics/undistorted_chessboard.PNG \"Undistorted\"\n",
    "[image2]: ./Pics/undistorted_lane.PNG \"Road Transformed\"\n",
    "[image3]: ./Pics/feature_map1.PNG \"Feature Map 1\"\n",
    "[image3b]: ./Pics/feature_map3.PNG \"Feature Map 2\"\n",
    "[image4]: ./Pics/perspective_transform.PNG \"Warp Example\"\n",
    "[image5]: ./Pics/feature_map1.PNG \"Fit Visual 1\"\n",
    "[image5b]: ./Pics/centroids.PNG \"Fit Visual 2\"\n",
    "[image5c]: ./Pics/trusted_points.PNG \"Fit Visual 3\"\n",
    "[image5d]: ./Pics/detected_lanes.PNG \"Fit Visual 4\"\n",
    "[image5e]: ./Pics/top_view1.PNG \"Fit Visual 5\"\n",
    "[image6]: ./Pics/complete_pic2.PNG \"Output\"\n",
    "[video1]: ./solution_project.mp4 \"Video\"\n",
    "\n",
    "## [Rubric](https://review.udacity.com/#!/rubrics/571/view) Points\n",
    "\n",
    "### Here I will consider the rubric points individually and describe how I addressed each point in my implementation.  \n",
    "\n",
    "---\n",
    "\n",
    "### Writeup / README\n",
    "\n",
    "All of my code can be found in my IPython notebook in \"./Write up.ipynb\" . In the following, I will consider all rubric points.\n",
    "\n",
    "### Camera Calibration\n",
    "\n",
    "#### 1. Calibration and Distortion:\n",
    "\n",
    "The code for this step is contained in the second code cell of my notebook. My approach was the one presented in the lecture. Mainly, I used the provided pictures of chessboards and the cv2-functions findChessboardCorners and calibrateCamera to calibrate the camera.\n",
    "\n",
    "I applied this distortion correction to the chessboard images and obtained the following result: \n",
    "\n",
    "![alt text][image1]\n",
    "\n",
    "\n",
    "### Pipeline (single images)\n",
    "\n",
    "#### 1. Provide an example of a distortion-corrected image.\n",
    "\n",
    "The next image illustrates the distortion correction:\n",
    "![alt text][image2]\n",
    "\n",
    "#### 2. Describe how (and identify where in your code) you used color transforms, gradients or other methods to create a thresholded binary image.  Provide an example of a binary image result.\n",
    "\n",
    "My pipeline consists of several steps where I first try high-thresholded filters to find lane line, and if this does not work, continue with lower-thresholded filters. In total, I used the S-channel and the R-channel (mainly to detect yellow lines), the grayscale (mainly to detect white lines), and also the Sobel-operator in 'x'-direction as well as the magnitude of the sobel operator.\n",
    "\n",
    "In the following pictures, we can see that more and more features are recognizeable. Also, features were only considered if they had a minimum number of white pixels. Additionally, I used a median filter to sharpen edges.\n",
    " \n",
    " \n",
    "![alt text][image3]\n",
    "![alt text][image3b]\n",
    "\n",
    "#### 3. Describe how (and identify where in your code) you performed a perspective transform and provide an example of a transformed image.\n",
    "\n",
    "Given the calibration matrix and distortion parameters, I used the picture 'straight_lines2.jpg' to calibrate the perspective transform. I chose the following source and destination points of my rectangles manually:\n",
    "\n",
    "Position    | Source        | Destination   | \n",
    ":----------:|:-------------:|:-------------:| \n",
    "lower_left  | 280, 678      | 300, 719      | \n",
    "lower_right | 1040, 678     | 900, 719      |\n",
    "upper_right | 705, 460      | 900, 0        |\n",
    "upper_left  | 580, 460      | 300, 0        |\n",
    "\n",
    "\n",
    "![alt text][image4]\n",
    "\n",
    "#### 4. Describe how (and identify where in your code) you identified lane-line pixels and fit their positions with a polynomial?\n",
    "\n",
    "In the first frame, I used the sliding window technique given in the lecture. Then, I fitted a weighted second order poylnomial to the lane lines.\n",
    "\n",
    "For the following frames, I tried to find window centroids. Here, I also did this consecutively for every of my chosen features and combined them. Then, I fitted a weighted second order polynomial to the center points of the windows.\n",
    "\n",
    "Some more tricks I used:\n",
    "- I saved the bottom point of the left and right lane in a global variable and started the window-centroid method from that point.\n",
    "- Windows that were too far from the last found window were discarded.\n",
    "- I computed two 'safe lines' around each lane and used them as a mask.\n",
    "- If a lane consisted of few points, I added three more points from the polynomial from the last iteration.\n",
    "- If a lane consisted of few points, but the other lane had a confident fit, I transformed points from the confident lane to the other.\n",
    "- I used a moving average over the parameters of the polynomials to smoothen the result.\n",
    "\n",
    "In the first picture, we can see a feature map.\n",
    "\n",
    "![alt text][image5]\n",
    "\n",
    "Next, the window centroids are shown:\n",
    "![alt text][image5b]\n",
    "\n",
    "Now, we can see the trusted points (red). We can observe that some points of the left lane are transferred to the right lane:\n",
    "![alt text][image5c]\n",
    "\n",
    "Finally, the lane is drawn (yellow). The blue crosses are points of the lane that are transported to the next frame for more stability.\n",
    "![alt text][image5d]\n",
    "\n",
    "\n",
    "The next picutre shows the final top view, cluding including the blue found lanes, the red bottom points and the search area for lane pixels in the next frame (marked by the red safe lines).\n",
    "\n",
    "![alt text][image5e]\n",
    "\n",
    "\n",
    "\n",
    "#### 5. Describe how (and identify where in your code) you calculated the radius of curvature of the lane and the position of the vehicle with respect to center.\n",
    "\n",
    "I did this analogously to the lecture and the code can be found in my Ipython notebook.\n",
    "\n",
    "#### 6. Provide an example image of your result plotted back down onto the road such that the lane area is identified clearly.\n",
    "\n",
    "The following pictures represent my pipeline. The green area marks the area between the lanes. The blue lines are the found lanes. The red lines are the 'safe lines' around the blue lanes. The red circles mark the 'bottom point' of each lane.\n",
    "\n",
    "\n",
    "![alt text][image6]\n",
    "\n",
    "---\n",
    "\n",
    "### Pipeline (video)\n",
    "\n",
    "#### 1. Provide a link to your final video output.  Your pipeline should perform reasonably well on the entire project video (wobbly lines are ok but no catastrophic failures that would cause the car to drive off the road!).\n",
    "\n",
    "Here's a [link to my video result](./solution_project.mp4)\n",
    "\n",
    "---\n",
    "\n",
    "### Discussion\n",
    "\n",
    "#### 1. Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?\n",
    "\n",
    "I found that my pipeline fits the project video but does not generalize so well to the other two videos. Here, the algorithm had some difficulties with shadows and sometimes recognized the shadows as lane lines. Also, using a moving average to smooth the lanes resulted in less variable curves. Maybe, one could use a better idea here.\n",
    "\n",
    "I order to improve my pipeline, I could try out more features, especially to prevent fitting on shadow lines. Also, I noticed that I used lots of 'hard-coded' features. Maybe a machine learning approach would work better here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
